PH·∫¶N 1 C√†i driver
---------------------------------------------------------------
sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
ubuntu-drivers devices


ch·ªçn b·∫£n cao nh·∫•t v√≠ d·ª•:

sudo apt install nvidia-driver-570-open
sudo reboot

# verify GPU ho·∫°t ƒë·ªông
nvidia-smi


ph·∫£i th·∫•y GPU + VRAM


PH·∫¶N 2 ‚Äî C√†i Ollama (LLM engine)
---------------------------------------------------------------
curl -fsSL https://ollama.com/install.sh | sh


run server

ollama serve


test:

curl http://localhost:11434


PH·∫¶N 3 ‚Äî Pull model ƒë·ªÉ test
---------------------------------------------------------------
Model nh·∫π test nhanh tr∆∞·ªõc:

ollama pull llama3


test chat:

ollama run llama3


n·∫øu GPU ho·∫°t ƒë·ªông ƒë√∫ng ‚Üí load s·∫Ω c·ª±c nhanh


PH·∫¶N 4 ‚Äî Pull model m·∫°nh h∆°n ƒë·ªÉ test GPU
---------------------------------------------------------------
RTX 5090 ‚Üí ch·∫°y tho·∫£i m√°i model l·ªõn

g·ª£i √Ω:

ollama pull mistral-large


ho·∫∑c

ollama pull mixtral


PH·∫¶N 5 ‚Äî C√†i Web UI chat (r·∫•t n√™n)
---------------------------------------------------------------
UI ƒë·∫πp + multi user + API key + history

docker run -d \
  -p 3000:8080 \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main


m·ªü browser:

http://localhost:3000


login ‚Üí ch·ªçn model ‚Üí chat


PH·∫¶N 6 ‚Äî Test load GPU th·∫≠t
---------------------------------------------------------------
m·ªü terminal kh√°c:

watch -n1 nvidia-smi


chat ‚Üí th·∫•y VRAM + compute tƒÉng = OK


PH·∫¶N 7 ‚Äî Benchmark nhanh GPU m·ªõi
---------------------------------------------------------------
test t·ªëc ƒë·ªô token:

ollama run llama3 "Write a 2000 word essay about AI"


quan s√°t:

Metric	t·ªët
load time	<3s
first token	<1s
tokens/sec	>80
PH·∫¶N 8 ‚Äî API call test (gi·ªëng OpenAI format)
curl http://localhost:11434/api/generate \
  -d '{
    "model":"llama3",
    "prompt":"Hello"
  }'


‚Üí b·∫°n c√≥ th·ªÉ d√πng endpoint n√†y cho app n·ªôi b·ªô

‚ö° Tuning ƒë·ªÉ GPU ch·∫°y max

file config:

~/.ollama/config


th√™m:

OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_LOADED_MODELS=2

üß† Model n√™n d√πng v·ªõi 5090 (khuy·∫øn ngh·ªã)
m·ª•c ƒë√≠ch	model
chat	llama3 70b q4
coding	deepseek coder
reasoning	mixtral
RAG	mistral
