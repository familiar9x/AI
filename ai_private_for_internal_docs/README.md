# Private RAG System v3

H·ªá th·ªëng RAG (Retrieval-Augmented Generation) ho√†n to√†n private, ch·∫°y 100% local trong m·∫°ng n·ªôi b·ªô.

## ‚ú® T√≠nh nƒÉng v3 (Latest)

- **üîê Enterprise SSO**: Keycloak OIDC/OAuth2 authentication
- **üë• AD Group Authorization**: Control access theo AD groups
- **üîí JWT Verification**: Backend verify JWT tokens th·∫≠t
- **üåê Nginx Reverse Proxy**: Unified HTTPS entry point
- **üé´ oauth2-proxy**: Transparent SSO cho UI v√† API

## ‚ú® T√≠nh nƒÉng v2

- **üìÑ Page-level citations**: Tr√≠ch d·∫´n ch√≠nh x√°c `[file.pdf | page X]`
- **‚ö° OCR cache**: Cache k·∫øt qu·∫£ OCR theo hash, ingest l·∫°i c·ª±c nhanh
- **üéØ Incremental ingest**: Ingest t·ª´ng file/folder ri√™ng l·∫ª, kh√¥ng c·∫ßn qu√©t to√†n b·ªô
- **üîç Per-page indexing**: M·ªói trang PDF l√† m·ªôt ƒë∆°n v·ªã ƒë·ªôc l·∫≠p

## Ki·∫øn tr√∫c

```
Docs (pdf/docx/md/txt)
   ‚Üì (ingest + chunk)
Embedding model (local)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  Vector DB (Qdrant)
   ‚Üì                                 ‚Üë
Query text ‚Üí embed ‚Üí retrieve topK ‚îÄ‚îÄ‚îÄ‚îò
   ‚Üì
LLM server (local, vLLM) ‚Üí generate answer + citations
   ‚Üì
Backend API (FastAPI)  + (optional) Redis cache
   ‚Üì
Web UI (OpenWebUI) ho·∫∑c UI t·ªëi gi·∫£n
```

## Stack c√¥ng ngh·ªá

- **LLM serving**: vLLM (OpenAI-compatible API)
- **Embedding**: sentence-transformers (bge-m3)
- **Vector DB**: Qdrant
- **API**: FastAPI
- **UI**: Open WebUI
- **Auth**: Keycloak OIDC (v3) with oauth2-proxy
- **Reverse Proxy**: Nginx with HTTPS
- **OCR**: Tesseract + pdf2image (x·ª≠ l√Ω PDF scan)

## Y√™u c·∫ßu h·ªá th·ªëng

- Docker & Docker Compose
- NVIDIA GPU (cho vLLM)
- NVIDIA Container Toolkit
- T·ªëi thi·ªÉu 16GB RAM (32GB+ khuy·∫øn ngh·ªã)
- 20GB+ dung l∆∞·ª£ng ƒëƒ©a cho models

## C√†i ƒë·∫∑t NVIDIA Container Toolkit

```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

## T·∫£i model LLM

T·∫£i model Qwen2.5-7B-Instruct (ho·∫∑c model kh√°c) v√†o th∆∞ m·ª•c `models/`:

```bash
# S·ª≠ d·ª•ng huggingface-cli
pip install huggingface-hub

huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ./models/Qwen2.5-7B-Instruct

# Ho·∫∑c t·∫£i th·ªß c√¥ng t·ª´ https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
```

## C·∫•u h√¨nh

### Quick Start (v2 - No SSO)

1. Ch·ªânh s·ª≠a file `.env`:

```bash
# Thay ƒë·ªïi API key (quan tr·ªçng!)
API_KEY=your_secure_random_key_here

# C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh embedding model
EMBED_MODEL=sentence-transformers/bge-m3

# ƒêi·ªÅu ch·ªânh RAG parameters
CHUNK_SIZE=900
CHUNK_OVERLAP=150
TOP_K=6

# OCR language: eng (English), vie (Vietnamese), ho·∫∑c eng+vie
OCR_LANG=eng+vie
```

2. Comment out oauth2-proxy v√† nginx trong `docker-compose.yml` n·∫øu kh√¥ng d√πng SSO

### Enterprise Setup (v3 - With SSO)

**Prerequisites:**
- Keycloak server ƒë√£ setup (external)
- SSL certificates
- DNS records

**Configuration:**

1. Setup Keycloak (xem [V3_SSO_GUIDE.md](docs/V3_SSO_GUIDE.md)):
   - Create realm
   - Create client `rag-proxy`
   - Create groups: `rag-admins`, `rag-users`
   - Configure group mapper

2. Generate secrets:
```bash
# Cookie secret
python -c "import os,base64; print(base64.b64encode(os.urandom(32)).decode())"
```

3. Update `.env` v·ªõi OIDC config (xem `.env` file)

4. Setup SSL certificates:
```bash
cd nginx/certs
# Copy ho·∫∑c generate certificates
# fullchain.pem, privkey.pem
```

5. Update `nginx/conf.d/default.conf` v·ªõi server name c·ªßa b·∫°n

2. N·∫øu d√πng model kh√°c, c·∫≠p nh·∫≠t `docker-compose.yml`:

```yaml
vllm:
  command: >
    --model /models/your-model-name
    --host 0.0.0.0
    --port 8000
    --dtype auto
    --max-model-len 8192
```

## Kh·ªüi ƒë·ªông h·ªá th·ªëng

### v2 Mode (No SSO)

```bash
# Build v√† start t·∫•t c·∫£ services
docker-compose up -d

# Xem logs
docker-compose logs -f

# Ki·ªÉm tra tr·∫°ng th√°i
docker-compose ps
```

**Access:**
- UI: http://localhost:3000
- API: http://localhost:8080
- Qdrant: http://localhost:6333

### v3 Mode (With SSO)

```bash
# Build v√† start
docker-compose up -d --build

# Check services
docker-compose ps

# Monitor logs
docker-compose logs -f nginx oauth2-proxy backend
```

**Access:**
- UI: https://rag.company.local (redirects to Keycloak login)
- API: https://rag.company.local/api
- Qdrant: http://localhost:6333 (internal only)

## S·ª≠ d·ª•ng

### 1. Th√™m t√†i li·ªáu

ƒê·∫∑t c√°c file t√†i li·ªáu v√†o th∆∞ m·ª•c `docs/`:

```bash
cp /path/to/your/documents/*.pdf docs/
cp /path/to/your/documents/*.docx docs/
```

**L∆∞u √Ω v·ªÅ PDF scan:**
- H·ªá th·ªëng t·ª± ƒë·ªông ph√°t hi·ªán PDF scan (√≠t text) v√† d√πng OCR
- PDF scan s·∫Ω x·ª≠ l√Ω ch·∫≠m h∆°n (OCR t·ªën CPU)
- H·ªó tr·ª£ ti·∫øng Vi·ªát: ƒë·∫∑t `OCR_LANG=vie` trong `.env`
- Ch·∫•t l∆∞·ª£ng OCR ph·ª• thu·ªôc v√†o ƒë·ªô ph√¢n gi·∫£i scan (khuy·∫øn ngh·ªã ‚â•250 DPI)

### 2. Ingest t√†i li·ªáu v√†o vector database

**‚ú® v2: Incremental Ingest**

```bash
# Ingest to√†n b·ªô docs/
curl -X POST "http://localhost:8080/admin/ingest" \
  -H "Authorization: Bearer your_secure_random_key_here"

# Ingest m·ªôt file c·ª• th·ªÉ
curl -X POST "http://localhost:8080/admin/ingest?path=handbook.pdf" \
  -H "Authorization: Bearer your_secure_random_key_here"

# Ingest m·ªôt folder con
curl -X POST "http://localhost:8080/admin/ingest?path=policies/" \
  -H "Authorization: Bearer your_secure_random_key_here"

# Ingest file v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi (trong container)
curl -X POST "http://localhost:8080/admin/ingest?path=/app/docs/reports/2024.pdf" \
  -H "Authorization: Bearer your_secure_random_key_here"
```

**Ch·∫°y tr·ª±c ti·∫øp trong container:**

```bash
# To√†n b·ªô
docker exec -it rag-backend python ingest.py

# M·ªôt file
docker exec -it rag-backend python ingest.py handbook.pdf

# M·ªôt folder
docker exec -it rag-backend python ingest.py policies/
```

**üí° L·ª£i √≠ch incremental ingest:**
- C·∫≠p nh·∫≠t nhanh khi th√™m t√†i li·ªáu m·ªõi
- Kh√¥ng c·∫ßn re-index to√†n b·ªô
- OCR cache gi·ªØ l·∫°i ‚Üí ingest l·∫°i file c≈© c·ª±c nhanh
- H·ªØu √≠ch khi c√≥ h√†ng ngh√¨n t√†i li·ªáu

### 3. Truy c·∫≠p Web UI

M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p:

```
http://localhost:3000
```

OpenWebUI s·∫Ω t·ª± ƒë·ªông k·∫øt n·ªëi v·ªõi backend RAG c·ªßa b·∫°n.

### 4. Test API tr·ª±c ti·∫øp

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_secure_random_key_here" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "user", "content": "C√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y"}
    ],
    "temperature": 0.2,
    "max_tokens": 512
  }'
```

**‚ú® v2: Response s·∫Ω c√≥ `rag_meta` v·ªõi page numbers:**

```json
{
  "choices": [...],
  "rag_meta": {
    "request_id": "abc123",
    "retrieved": [
      {
        "source": "/app/docs/handbook.pdf",
        "page_number": 15,
        "chunk_index": 2,
        "score": 0.87
      },
      ...
    ],
    "latency_ms": 234
  }
}
```

## Services v√† Ports

- **Qdrant**: http://localhost:6333 (Vector database admin UI)
- **vLLM**: http://localhost:8000 (LLM inference API)
- **Backend**: http://localhost:8080 (RAG API)
- **OpenWebUI**: http://localhost:3000 (Web interface)

## Health checks

```bash
# Backend health
curl http://localhost:8080/healthz

# vLLM health
curl http://localhost:8000/health

# Qdrant health
curl http://localhost:6333/healthz
```

## Qu·∫£n l√Ω

### D·ª´ng h·ªá th·ªëng

```bash
docker-compose down
```

### X√≥a t·∫•t c·∫£ data (bao g·ªìm vector database)

```bash
docker-compose down -v
```

### Restart m·ªôt service

```bash
docker-compose restart backend
docker-compose restart vllm
```

### Xem logs c·ªßa m·ªôt service

```bash
docker-compose logs -f backend
docker-compose logs -f vllm
```

## Troubleshooting

### GPU kh√¥ng ƒë∆∞·ª£c nh·∫≠n di·ªán

```bash
# Ki·ªÉm tra NVIDIA driver
nvidia-smi

# Ki·ªÉm tra Docker c√≥ th·∫•y GPU kh√¥ng
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### vLLM kh√¥ng start ƒë∆∞·ª£c

- Ki·ªÉm tra GPU memory: model c·∫ßn √≠t nh·∫•t 8GB VRAM
- Gi·∫£m `--max-model-len` trong docker-compose.yml
- Th·ª≠ model nh·ªè h∆°n (v√≠ d·ª•: Qwen2.5-3B)

### Backend kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Qdrant/vLLM

- ƒê·ª£i v√†i ph√∫t ƒë·ªÉ c√°c service kh·ªüi ƒë·ªông ho√†n to√†n
- Ki·ªÉm tra logs: `docker-compose logs qdrant vllm`
- Ki·ªÉm tra network: `docker network inspect private-rag_default`

### Embedding model download ch·∫≠m

Embedding model s·∫Ω t·ª± ƒë·ªông download l·∫ßn ƒë·∫ßu ti√™n. ƒê·ªÉ tƒÉng t·ªëc:

```bash
# Pre-download embedding model
docker exec -it rag-backend python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/bge-m3')"
```

### OCR kh√¥ng ho·∫°t ƒë·ªông ho·∫∑c k·∫øt qu·∫£ k√©m

**PDF kh√¥ng ƒë∆∞·ª£c OCR:**
- Ki·ªÉm tra PDF c√≥ √≠t h∆°n 200 k√Ω t·ª± text kh√¥ng (ng∆∞·ª°ng trigger OCR)
- Xem logs: `docker-compose logs backend | grep OCR`

**K·∫øt qu·∫£ OCR k√©m ch·∫•t l∆∞·ª£ng:**
- PDF scan c√≥ ƒë·ªô ph√¢n gi·∫£i th·∫•p (< 200 DPI)
- Ch·ªØ trong PDF b·ªã m√©o, m·ªù
- Gi·∫£i ph√°p: re-scan t√†i li·ªáu v·ªõi DPI cao h∆°n (‚â•250 DPI)

**OCR ti·∫øng Vi·ªát kh√¥ng ƒë√∫ng:**
- ƒê·∫£m b·∫£o ƒë√£ set `OCR_LANG=vie` trong `.env`
- Rebuild backend: `docker-compose up -d --build backend`
- Tesseract language pack ƒë√£ ƒë∆∞·ª£c c√†i trong Dockerfile

**OCR qu√° ch·∫≠m:**
- OCR t·ªën CPU, b√¨nh th∆∞·ªùng v·ªõi PDF scan nhi·ªÅu trang
- Xem x√©t tƒÉng CPU limits trong docker-compose n·∫øu c·∫ßn
- C√≥ th·ªÉ pre-process PDFs offline tr∆∞·ªõc khi deploy

## T·ªëi ∆∞u h√≥a

### TƒÉng performance

1. **TƒÉng s·ªë l∆∞·ª£ng retrieve chunks**:
   ```bash
   # Trong .env
   TOP_K=10
   ```

2. **ƒêi·ªÅu ch·ªânh chunk size**:
   ```bash
   CHUNK_SIZE=1200
   CHUNK_OVERLAP=200
   ```

3. **Enable quantization cho vLLM**:
   ```yaml
   # Trong docker-compose.yml
   command: >
     --model /models/Qwen2.5-7B-Instruct
     --quantization awq
   ```

### Gi·∫£m memory usage

1. D√πng model nh·ªè h∆°n
2. Gi·∫£m `--max-model-len`
3. Enable CPU offload (n·∫øu RAM nhi·ªÅu h∆°n VRAM)

## Production checklist

- [ ] ƒê·ªïi `API_KEY` th√†nh gi√° tr·ªã ng·∫´u nhi√™n m·∫°nh
- [ ] Setup HTTPS reverse proxy (nginx/traefik)
- [ ] Enable rate limiting
- [ ] Setup monitoring (Prometheus + Grafana)
- [ ] Setup backup cho Qdrant data
- [ ] Configure log rotation
- [ ] Setup alerting
- [ ] Document disaster recovery procedure
- [ ] Regular security updates

## N√¢ng c·∫•p t∆∞∆°ng lai

- [ ] Redis cache cho frequently asked questions
- [ ] OAuth/SSO integration
- [ ] Multi-user support v·ªõi user quotas
- [ ] Advanced RAG: hybrid search (dense + sparse)
- [ ] Re-ranking model
- [ ] Query expansion/rewriting
- [ ] Feedback loop ƒë·ªÉ improve results
- [ ] A/B testing framework

## License

Internal use only.

## Support

Contact your internal DevOps/ML team for support.
