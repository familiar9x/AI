# This directory stores your LLM models

Place your downloaded models here. For example:

```bash
# Download Qwen2.5-7B-Instruct
huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ./Qwen2.5-7B-Instruct
```

## Recommended models

### For production (with GPU):
- **Qwen2.5-7B-Instruct** - Good balance of quality and speed
- **Qwen2.5-14B-Instruct** - Higher quality, needs more VRAM
- **Mistral-7B-Instruct-v0.3** - Alternative option

### For testing (smaller):
- **Qwen2.5-3B-Instruct** - Faster, lower memory
- **Phi-3-mini-4k-instruct** - Microsoft's small model

## Model structure

After download, your structure should look like:

```
models/
  Qwen2.5-7B-Instruct/
    config.json
    tokenizer.json
    *.safetensors
    ...
```

Update the model path in `docker-compose.yml` to match your downloaded model.
